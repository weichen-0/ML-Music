{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "\n",
    "Jigs (340 tunes) & Hornpipes (65 tunes) from http://abc.sourceforge.net/NMD/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Data/'\n",
    "data_file = \"Music_Data.txt\"\n",
    "char_index_json = 'char_to_index.json'\n",
    "model_weights_dir = './Data/Model_Weights/'\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LENGTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batches(all_chars, unique_chars):\n",
    "    length = all_chars.shape[0]\n",
    "    batch_chars = int(length / BATCH_SIZE)\n",
    "    \n",
    "    for start in range(0, batch_chars - SEQ_LENGTH, 64):\n",
    "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))\n",
    "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, unique_chars))\n",
    "        \n",
    "        for batch_index in range(BATCH_SIZE):\n",
    "            for seq_index in range(SEQ_LENGTH):\n",
    "                X[batch_index, seq_index] = all_chars[batch_index * batch_chars + start + seq_index]\n",
    "                Y[batch_index, seq_index, all_chars[batch_index * batch_chars + start + seq_index + 1]] = 1\n",
    "        \n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(batch_size, seq_length, unique_chars):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim = unique_chars, output_dim = 512, batch_input_shape=(batch_size, seq_length)))\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(unique_chars)))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "#     For future model training\n",
    "#     model.load_weights(\"./Data/Model_Weights/Weights_80.h5\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, epochs=90):\n",
    "    char_to_index = {ch: i for (i, ch) in enumerate(sorted(list(set(data))))}\n",
    "    print(\"Number of unique characters in our whole music database = {}\".format(len(char_to_index)))\n",
    "    \n",
    "    with open(os.path.join(data_dir, char_index_json), mode='w') as f:\n",
    "        json.dump(char_to_index, f)\n",
    "        \n",
    "    index_to_char = {i: ch for (ch, i) in char_to_index.items()}\n",
    "    unique_chars = len(char_to_index)\n",
    "    \n",
    "    model = create_model(BATCH_SIZE, SEQ_LENGTH, unique_chars)\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    all_characters = np.asarray([char_to_index[c] for c in data], dtype=np.int32)\n",
    "    print(\"Total number of characters =\", all_characters.shape[0])\n",
    "    \n",
    "    epoch_number, loss, acc = [], [], []\n",
    "    \n",
    "    existing_weight_count = 0\n",
    "    \n",
    "#     For future model training: determines weight file naming\n",
    "#     while True:\n",
    "#         existing_weight = model_weights_dir +\"Weights_{}.h5\".format(existing_weight_count)\n",
    "#         if (os.path.exists(existing_weight)):\n",
    "#             existing_weight_count += 10\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, epochs))\n",
    "        final_epoch_loss, final_epoch_acc = 0, 0\n",
    "        epoch_number.append(epoch + 1)\n",
    "        \n",
    "        for i, (x, y) in enumerate(read_batches(all_characters, unique_chars)):\n",
    "            final_epoch_loss, final_epoch_acc = model.train_on_batch(x, y)\n",
    "            print(\"Batch: {}, Loss: {}, Accuracy: {}\".format(i+1, final_epoch_loss, final_epoch_acc))\n",
    "        \n",
    "        loss.append(final_epoch_loss)\n",
    "        acc.append(final_epoch_acc)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            if not os.path.exists(model_weights_dir):\n",
    "                os.makedirs(model_weights_dir)\n",
    "            \n",
    "            model.save_weights(os.path.join(model_weights_dir, \"Weights_{}.h5\".format(epoch + existing_weight_count + 1)))\n",
    "            print('Saved Weights at epoch {} to file Weights_{}.h5'.format(epoch + existing_weight_count + 1, epoch + existing_weight_count + 1))\n",
    "    \n",
    "    log_frame = pd.DataFrame(columns=['Epoch', 'Loss', 'Accuracy'])\n",
    "    log_frame['Epoch'] = epoch_number\n",
    "    log_frame['Loss'] = loss\n",
    "    log_frame['Accuracy'] = acc\n",
    "    log_frame.to_csv(\"./Data/log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = open(os.path.join(data_dir, data_file), mode='r')\n",
    "data = file.read()\n",
    "file.close()\n",
    "if __name__ == '__main__':\n",
    "    train_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           loss  accuracy\n",
      "epoch                    \n",
      "1      2.668069  0.263672\n",
      "2      1.919861  0.454102\n",
      "3      1.621433  0.538086\n",
      "4      1.447527  0.563477\n",
      "5      1.321654  0.582031\n",
      "6      1.268126  0.602539\n",
      "7      1.178231  0.625977\n",
      "8      1.145388  0.635742\n",
      "9      1.078966  0.657227\n",
      "10     1.040754  0.660156\n",
      "11     0.994467  0.682617\n",
      "12     0.983068  0.698242\n",
      "13     0.941521  0.707031\n",
      "14     0.909094  0.707031\n",
      "15     0.880224  0.719727\n",
      "16     0.845350  0.727539\n",
      "17     0.844031  0.727539\n",
      "18     0.805950  0.749023\n",
      "19     0.808790  0.733398\n",
      "20     0.767415  0.751953\n",
      "21     0.753273  0.761719\n",
      "22     0.736319  0.761719\n",
      "23     0.738621  0.764648\n",
      "24     0.732230  0.754883\n",
      "25     0.709934  0.776367\n",
      "26     0.677853  0.774414\n",
      "27     0.671902  0.776367\n",
      "28     0.646896  0.790039\n",
      "29     0.644653  0.787109\n",
      "30     0.627049  0.790039\n",
      "...         ...       ...\n",
      "61     0.408580  0.868164\n",
      "62     0.369339  0.877930\n",
      "63     0.378671  0.881836\n",
      "64     0.337536  0.894531\n",
      "65     0.370667  0.877930\n",
      "66     0.375977  0.876953\n",
      "67     0.383287  0.879883\n",
      "68     0.350804  0.874023\n",
      "69     0.378644  0.887695\n",
      "70     0.360039  0.872070\n",
      "71     0.346715  0.893555\n",
      "72     0.361582  0.886719\n",
      "73     0.356573  0.883789\n",
      "74     0.363088  0.882812\n",
      "75     0.360034  0.884766\n",
      "76     0.324683  0.894531\n",
      "77     0.351364  0.891602\n",
      "78     0.335319  0.895508\n",
      "79     0.326502  0.890625\n",
      "80     0.349983  0.882812\n",
      "81     0.328770  0.896484\n",
      "82     0.315892  0.899414\n",
      "83     0.301607  0.903320\n",
      "84     0.330237  0.897461\n",
      "85     0.338864  0.887695\n",
      "86     0.352604  0.883789\n",
      "87     0.295191  0.899414\n",
      "88     0.331248  0.881836\n",
      "89     0.331918  0.887695\n",
      "90     0.287388  0.900391\n",
      "\n",
      "[90 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "log = pd.read_csv(os.path.join(data_dir, \"Model_Training/training_data.csv\"), index_col='epoch')\n",
    "print(log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
